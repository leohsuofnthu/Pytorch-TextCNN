{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN_IMDB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohsuofnthu/Pytorch-TextCNN/blob/master/textCNN_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDi2Zu6Wduss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from numpy.random import RandomState\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import torchtext.datasets\n",
        "from torchtext.datasets import IMDB, SST\n",
        "\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3b8tR4hUI1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Split whole dataset into train and valid set\n",
        "df = pd.read_csv('./IMDB_Dataset.csv')\n",
        "rng = RandomState()\n",
        "\n",
        "tr = df.sample(frac=0.8, random_state=rng)\n",
        "tst = df.loc[~df.index.isin(tr.index)]\n",
        "tr.to_csv('train.csv', index=False)\n",
        "tst.to_csv('valid.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLibIFbDUmTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Prepare the dataset via torchtext\n",
        "spacy_en = spacy.load('en', disable=['tagger', 'parser', 'ner', 'textcat'\n",
        "                                     'entity_ruler', 'sentencizer', \n",
        "                                     'merge_noun_chunks', 'merge_entities',\n",
        "                                     'merge_subtokens'])\n",
        "\n",
        "def tokenizer(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "  \n",
        "# set up fields\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#Creating field for text and label\n",
        "TEXT = Field(sequential=True, tokenize=tokenizer, lower=True)\n",
        "LABEL = Field(sequential=False)\n",
        "\n",
        "#clean the text\n",
        "TEXT.preprocessing = torchtext.data.Pipeline(clean_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y53KC7iV0rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "train_datafield = [('text', TEXT),  ('label', LABEL)]\n",
        "train = TabularDataset(path ='./train.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header=True,\n",
        "                             fields=train_datafield)\n",
        "\n",
        "\n",
        "#%%\n",
        "test_datafield = [('text', TEXT),  ('label',LABEL)]\n",
        "\n",
        "test = TabularDataset(path ='./valid.csv', \n",
        "                       format='csv',\n",
        "                       skip_header=True,\n",
        "                       fields=test_datafield)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4hEm_F-uC8C",
        "colab_type": "code",
        "outputId": "d1d05829-4707-4207-9e9f-8ea1d26e04a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#%%Show some example to show the dataset\n",
        "print(train[0].text,  train[0].label)\n",
        "print(test[0].text,  test[0].label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['crackerjack', 'is', 'a', 'simple', 'but', 'feelgood', 'movie', 'where', 'the', 'good', 'guys', 'are', 'very', 'good', 'and', 'the', 'bad', 'guys', 'are', 'very', 'bad', 'and', 'the', 'central', 'character', 'is', 'tempted', 'by', 'both', 'sides br', 'br', 'the', 'combination', 'of', 'the', 'central', 'character', 'being', 'played', 'by', 'mick', 'malloy', 'and', 'the', 'central', 'setting', 'being', 'the', 'local', 'lawn', 'bowls', 'clubs', 'drew', 'an', 'unusually', 'broad', 'crowd', 'ranging', 'from', 'large', 'numbers', 'of', 'teenagers', 'to', 'large', 'numbers', 'of', 'senior', 'citizens', '', 'and', 'all', 'laughed', 'at', 'the', 'comedy br', 'br', 'as', 'would', 'be', 'expected', 'of', 'a', 'movie', 'with', 'mick', 'malloy', 'and', 'judith', 'lucy', 'there', 'was', 'quite', 'a', 'bit', 'of', 'swearing', ',', 'but', 'it', 'was', 'not', 'overdone', 'and', 'the', 'audience', 'i', 'sat', 'with', 'certainly', 'enjoyed', 'it ! br', 'br', 'mick', 'malloy', 'did', 'a', 'good', 'job', 'as', 'the', 'lazy', 'bloke', 'who', 'joined', 'the', 'bowls', 'club', '\\\\(', 'three', 'times', '\\\\)', 'simply', 'to', 'get', 'parking', 'spaces', '\\\\(', 'one', 'for', 'himself', 'and', 'two', 'for', 'leasing', 'to', 'others', 'at', 'a', 'premium', '\\\\)', 'but', 'who', 'has', 'everything', 'fall', 'down', 'on', 'him', 'when', 'he', 'is', 'required', 'to', 'play', 'or', 'lose', 'his', 'membership br', 'br', 'judith', 'lucy', 'does', 'a', 'fine', 'job', 'as', 'his', 'local', 'journalist', '', 'love', '', 'interest', 'and', 'there', 'are', 'fabulous', 'performances', 'from', 'bill', 'hunter', ',', 'frank', 'wilson', ',', 'monica', 'maughan', ',', 'lois', 'ramsey', 'and', 'many', 'others br', 'br', 'john', 'clarke', \"'s\", 'dour', 'role', 'as', 'the', 'bad', 'guy', 'is', 'not', 'one', 'of', 'his', 'funniest', 'but', 'he', 'gives', 'a', 'solid', 'performance br', 'br', 'the', 'not', 'so', 'subtle', 'swipes', 'at', 'pokies', 'provide', 'a', 'bit', 'of', 'a', 'serious', 'note', 'to', 'this', 'otherwise', 'light', 'comedy br', 'br', \"i'm\", 'sure', 'that', 'those', 'who', 'enjoyed', 'the', 'castle', 'and', 'the', 'dish', 'would', 'also', 'enjoy', 'this', 'movie', ''] positive\n",
            "['probably', 'my', 'all', '', 'time', 'favorite', 'movie', ',', 'a', 'story', 'of', 'selflessness', ',', 'sacrifice', 'and', 'dedication', 'to', 'a', 'noble', 'cause', ',', 'but', 'it', \"'s\", 'not', 'preachy', 'or', 'boring', '', 'it', 'just', 'never', 'gets', 'old', ',', 'despite', 'my', 'having', 'seen', 'it', 'some', '15', 'or', 'more', 'times', 'in', 'the', 'last', '25', 'years', '', 'paul', 'lukas', \"'\", 'performance', 'brings', 'tears', 'to', 'my', 'eyes', ',', 'and', 'bette', 'davis', ',', 'in', 'one', 'of', 'her', 'very', 'few', 'truly', 'sympathetic', 'roles', ',', 'is', 'a', 'delight', '', 'the', 'kids', 'are', ',', 'as', 'grandma', 'says', ',', 'more', 'like', '', 'dressed', '', 'up', 'midgets', '', 'than', 'children', ',', 'but', 'that', 'only', 'makes', 'them', 'more', 'fun', 'to', 'watch', '', 'and', 'the', 'mother', \"'s\", 'slow', 'awakening', 'to', 'what', \"'s\", 'happening', 'in', 'the', 'world', 'and', 'under', 'her', 'own', 'roof', 'is', 'believable', 'and', 'startling', '', 'if', 'i', 'had', 'a', 'dozen', 'thumbs', ',', 'they', \"'d\", 'all', 'be', '', 'up', '', 'for', 'this', 'movie', ''] positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0pRfczV41t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Check the dataset\n",
        "TEXT.build_vocab(train, vectors= 'glove.6B.300d')\n",
        "LABEL.build_vocab(train)\n",
        "#%% load the pretrained embedding\n",
        "vocab = TEXT.vocab\n",
        "\n",
        "#%% Create the Iterator for datasets (Iterator works like dataloader)\n",
        "\n",
        "train_iter = Iterator(\n",
        "        train, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = Iterator(test, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSjRipJEYm5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Text CNN model\n",
        "class textCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
        "        super(textCNN, self).__init__()\n",
        "        #load pretrained embedding in embedding layer.\n",
        "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
        "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
        "    \n",
        "        #Convolutional Layers with different window size kernels\n",
        "        print([w for w in kernel_wins])\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
        "        #Dropout layer\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        \n",
        "        #FC layer\n",
        "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb_x = self.embed(x)\n",
        "        emb_x = emb_x.unsqueeze(1)\n",
        "\n",
        "        con_x = [conv(emb_x) for conv in self.convs]\n",
        "\n",
        "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
        "        print(pool_x[0].size())\n",
        "        \n",
        "        fc_x = torch.cat(pool_x, dim=1)\n",
        "        \n",
        "        fc_x = fc_x.squeeze(-1)\n",
        "        print(fc_x.size())\n",
        "        fc_x = self.dropout(fc_x)\n",
        "        logit = self.fc(fc_x)\n",
        "        return logit\n",
        "        \n",
        "\n",
        "#%% Training the Model\n",
        "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
        "    model.train()\n",
        "    corrects, train_loss = 0.0,0\n",
        "    for batch in train_itr:\n",
        "        text, target = batch.text, batch.label\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logit = model(text)\n",
        "        \n",
        "        loss = F.cross_entropy(logit, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss+= loss.item()\n",
        "        result = torch.max(logit,1)[1]\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    \n",
        "    size = len(train_itr.dataset)\n",
        "    train_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "  \n",
        "    return train_loss, accuracy\n",
        "    \n",
        "def valid(model, device, test_itr):\n",
        "    model.eval()\n",
        "    corrects, test_loss = 0.0,0\n",
        "    for batch in test_itr:\n",
        "        text, target = batch.text, batch.label\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        \n",
        "        logit = model(text)\n",
        "        loss = F.cross_entropy(logit, target)\n",
        "\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "        result = torch.max(logit,1)[1]\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    \n",
        "    size = len(test_itr.dataset)\n",
        "    test_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNHg-VStdsrB",
        "colab_type": "code",
        "outputId": "5a9ba09e-b7a5-462c-c9e0-0f3e6b8ddaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "#%%\n",
        "model = textCNN(vocab, 300, 100, [3, 4 , 5] , 2).to('cuda')\n",
        "# print the model summery\n",
        "print(model)    \n",
        "    \n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1, 10+1):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc = valid(model, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(model.state_dict(), \"textCNN_IMDB_best_valid\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 4, 5]\n",
            "textCNN(\n",
            "  (embed): Embedding(119131, 300)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.6)\n",
            "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
            ")\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n",
            "torch.Size([64, 100, 1])\n",
            "torch.Size([64, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c0456bfdc0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Epoch: {} \\t Loss: {} \\t Accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f981f28eb62c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_itr, optimizer, epoch, max_epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvkgZzyvY6_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #plot train/validation loss versus epoch\n",
        "    x = list(range(1, 10))\n",
        "    plt.figure()\n",
        "    plt.title(\"train/validation loss versus epoch\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"total loss\")\n",
        "    plt.plot(x, train_loss,label=\"train loss\")\n",
        "    plt.plot(x, test_loss, color='red', label=\"test loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    #plot train/validation accuracy versus epoch\n",
        "    x = list(range(1, 10))\n",
        "    plt.figure()\n",
        "    plt.title(\"train/validation loss versus epoch\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"total loss\")\n",
        "    plt.plot(x, train_loss,label=\"train loss\")\n",
        "    plt.plot(x, test_loss, color='red', label=\"test loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYROIYRv3ZoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}