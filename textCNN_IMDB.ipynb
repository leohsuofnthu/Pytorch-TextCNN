{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN_IMDB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohsuofnthu/Pytorch-TextCNN/blob/master/textCNN_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDi2Zu6Wduss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from numpy.random import RandomState\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import torchtext.datasets\n",
        "from torchtext.datasets import IMDB, SST\n",
        "\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3b8tR4hUI1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Split whole dataset into train and valid set\n",
        "df = pd.read_csv('./IMDB_Dataset.csv')\n",
        "rng = RandomState()\n",
        "\n",
        "tr = df.sample(frac=0.8, random_state=rng)\n",
        "tst = df.loc[~df.index.isin(tr.index)]\n",
        "tr.to_csv('train.csv', index=False)\n",
        "tst.to_csv('valid.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLibIFbDUmTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Prepare the dataset via torchtext\n",
        "spacy_en = spacy.load('en', disable=['tagger', 'parser', 'ner', 'textcat'\n",
        "                                     'entity_ruler', 'sentencizer', \n",
        "                                     'merge_noun_chunks', 'merge_entities',\n",
        "                                     'merge_subtokens'])\n",
        "\n",
        "def tokenizer(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "  \n",
        "# set up fields\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#Creating field for text and label\n",
        "TEXT = Field(sequential=True, tokenize=tokenizer, lower=True)\n",
        "LABEL = Field(sequential=False)\n",
        "\n",
        "#clean the text\n",
        "TEXT.preprocessing = torchtext.data.Pipeline(clean_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y53KC7iV0rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "train_datafield = [('text', TEXT),  ('label', LABEL)]\n",
        "train = TabularDataset(path ='./train.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header=True,\n",
        "                             fields=train_datafield)\n",
        "\n",
        "\n",
        "#%%\n",
        "test_datafield = [('text', TEXT),  ('label',LABEL)]\n",
        "\n",
        "test = TabularDataset(path ='./valid.csv', \n",
        "                       format='csv',\n",
        "                       skip_header=True,\n",
        "                       fields=test_datafield)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4hEm_F-uC8C",
        "colab_type": "code",
        "outputId": "41873300-397c-451b-b0fe-87651ebab273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#%%Show some example to show the dataset\n",
        "print(train[0].text,  train[0].label)\n",
        "print(test[0].text,  test[0].label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', 'is', 'ben', 'shapiro', \"'s\", 'mbti', 'type', '\\\\?'] 0\n",
            "['do', 'you', 'have', 'an', 'adopted', 'dog', ',', 'how', 'would', 'you', 'encourage', 'people', 'to', 'adopt', 'and', 'not', 'shop', '\\\\?'] 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0pRfczV41t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Check the dataset\n",
        "TEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300))\n",
        "LABEL.build_vocab(train)\n",
        "#%% load the pretrained embedding\n",
        "vocab = REVIEW.vocab\n",
        "\n",
        "#%% Create the Iterator for datasets (Iterator works like dataloader)\n",
        "\n",
        "train_iter = Iterator(\n",
        "        train, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = Iterator(test, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSjRipJEYm5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Text CNN model\n",
        "class textCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
        "        super(textCNN, self).__init__()\n",
        "        #load pretrained embedding in embedding layer.\n",
        "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
        "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
        "    \n",
        "        #Convolutional Layers with different window size kernels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, w, emb_dim) for w in kernel_wins])\n",
        "        #Dropout layer\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        \n",
        "        #FC layer\n",
        "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb_x = self.embed(x)\n",
        "        emb_x = emb_x.unsqueeze(1)\n",
        "\n",
        "        con_x = [conv(emb_x) for conv in self.convs]\n",
        "        \n",
        "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
        "        \n",
        "        fc_x = torch.stack(pool_x, dim=1)\n",
        "        fc_x = fc_x.squeeze(-1)\n",
        "        fc_x = self.dropout(fc_x)\n",
        "        \n",
        "        logit = self.fc(fc_x)\n",
        "        return logit\n",
        "        \n",
        "\n",
        "#%% Training the Model\n",
        "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
        "    model.train()\n",
        "    corrects, train_loss = 0,0\n",
        "    for batch in train_itr:\n",
        "        text, target = batch.question_text, batch.target\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logit = model(text)\n",
        "        \n",
        "        loss = F.cross_entropy(logit, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss+= loss.item()\n",
        "        result = torch.max(logit,1)[1]\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    \n",
        "    size = len(train_itr.dataset)\n",
        "    train_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "  \n",
        "    return train_loss, accuracy\n",
        "    \n",
        "def eval(model, device, test_itr):\n",
        "    model.eval()\n",
        "    corrects, test_loss = 0,0\n",
        "    for batch in test_itr:\n",
        "        text, target = batch.question_text, batch.target\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        \n",
        "        logit = model(text)\n",
        "        loss = F.cross_entropy(logit, target)\n",
        "\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "        result = torch.max(logit,1)[1]\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    \n",
        "    size = len(test_itr.dataset)\n",
        "    test_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNHg-VStdsrB",
        "colab_type": "code",
        "outputId": "8bb170c5-0bac-4161-af2a-7a88723c072c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "#%%\n",
        "model = textCNN(vocab, 300, 100, [6, 12 , 24] , 2).to('cuda')\n",
        "# print the model summery\n",
        "print(model)    \n",
        "    \n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1, 10+1):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc = eval(model, device, test_iter)\n",
        "    print('Test Epoch: {} \\t Loss: {} \\t Accuracy: {}'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "textCNN(\n",
            "  (embed): Embedding(185314, 300)\n",
            "  (conv_w3): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
            "  (conv_w4): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
            "  (conv_w5): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
            "  (dropout): Dropout(p=0.6)\n",
            "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
            ")\n",
            "Train Epoch: 1 \t Loss: 0.0020322306527946767 \t Accuracy: 95\n",
            "Test Epoch: 1 \t Loss: 0.0017544562399427405 \t Accuracy: 95\n",
            "model saves at 95 accuracy\n",
            "Train Epoch: 2 \t Loss: 0.0017895910681491195 \t Accuracy: 95\n",
            "Test Epoch: 2 \t Loss: 0.001800485071341099 \t Accuracy: 95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d0f264a57324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Epoch: {} \\t Loss: {} \\t Accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2373c67b55ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_itr, optimizer, epoch, max_epoch)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mcorrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvkgZzyvY6_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #plot train/validation loss versus epoch\n",
        "    x = list(range(1, 10))\n",
        "    plt.figure()\n",
        "    plt.title(\"train/validation loss versus epoch\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"total loss\")\n",
        "    plt.plot(x, train_loss,label=\"train loss\")\n",
        "    plt.plot(x, test_loss, color='red', label=\"test loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    #plot train/validation accuracy versus epoch\n",
        "    x = list(range(1, 10))\n",
        "    plt.figure()\n",
        "    plt.title(\"train/validation loss versus epoch\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"total loss\")\n",
        "    plt.plot(x, train_loss,label=\"train loss\")\n",
        "    plt.plot(x, test_loss, color='red', label=\"test loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYROIYRv3ZoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}